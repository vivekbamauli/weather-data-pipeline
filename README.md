Real-Time Weather Data Pipeline
(Kafka â†’ Spark Structured Streaming â†’ PostgreSQL)

This project demonstrates a complete real-time data engineering pipeline using:

Apache Kafka for streaming ingestion

Spark Structured Streaming for real-time processing

PostgreSQL for storage

Python for orchestration

It reads live weather data from an API, publishes to Kafka, processes using Spark, and stores results into PostgreSQL.

ğŸ“Œ Features

âœ” Real-time weather data ingestion
âœ” Kafka producer & consumer architecture
âœ” Spark Structured Streaming transformation
âœ” JDBC write into PostgreSQL
âœ” Fully automated streaming workflow
âœ” Ready for deployment & extension

ğŸ› ï¸ Tech Stack
Component	Technology
Stream Producer	Python + Kafka
Stream Processor	Apache Spark 3.5.1
Message Broker	Apache Kafka
Database	PostgreSQL
Runtime	WSL2 + Windows
ğŸ“‚ Project Structure
weather_data_pipeline/
â”‚
â”œâ”€â”€ producer.py              # Fetch weather data â†’ Kafka
â”œâ”€â”€ spark_stream.py          # Spark Streaming (Kafka â†’ Postgres)
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ README.md                # Documentation

ğŸš€ How to Run This Project

Follow these steps carefully.

âœ… 1. Start Zookeeper & Kafka
Open WSL terminal â†’ go to Kafka directory
cd /mnt/c/kafka/kafka_2.12-3.2.3

Start Zookeeper
bin/zookeeper-server-start.sh config/zookeeper.properties

Open new terminal â†’ Start Kafka broker
cd /mnt/c/kafka/kafka_2.12-3.2.3
bin/kafka-server-start.sh config/server.properties

âœ… 2. Create Kafka Topic
cd /mnt/c/kafka/kafka_2.12-3.2.3

bin/kafka-topics.sh --create \
  --topic weather_topic \
  --bootstrap-server localhost:9092


Verify:

bin/kafka-topics.sh --list --bootstrap-server localhost:9092

âœ… 3. Start PostgreSQL
Start server
sudo service postgresql start

Enter psql
sudo -u postgres psql

Create database
CREATE DATABASE clickstream_db;

Connect to DB
\c clickstream_db;

Create table
CREATE TABLE weather_stream (
    lon FLOAT,
    lat FLOAT,
    temperature FLOAT,
    humidity INT,
    city VARCHAR(50),
    batch_time TIMESTAMP
);

âœ… 4. Install Dependencies

Inside your project folder:

pip install -r requirements.txt

âœ… 5. Start Kafka Weather Producer

This script fetches live weather data and sends it to Kafka every few seconds.

python3 producer.py

âœ… 6. Run Spark Streaming Job

Go to Spark folder:

cd ~/spark-3.5.1-bin-hadoop3


Run with PostgreSQL + Kafka JARs:

./bin/spark-submit \
  --jars jars-kafka/*,jars-kafka/postgresql-42.7.2.jar \
  "/mnt/c/Users/asus/OneDrive/Desktop/data pipeline/weather_data_pipeline/spark_stream.py"


Spark UI â†’ http://localhost:4040

ğŸ¯ Output

âœ” Weather data is produced to Kafka
âœ” Spark consumes it in real-time
âœ” JSON is transformed into columns
âœ” Final data is stored in PostgreSQL table weather_stream

View results:
SELECT * FROM weather_stream;

ğŸ“¦ requirements.txt
pyspark
requests
kafka-python
psycopg2-binary

ğŸ”® Future Enhancements

Add Apache Airflow orchestration

Migrate storage to Snowflake

Containerize with Docker

Add Power BI Dashboards

âœ¨ Author

Vivek Kumar
Data Engineer â€” Spark | Kafka | PostgreSQL | Azure
